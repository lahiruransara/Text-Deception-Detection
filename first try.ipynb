{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lahiru ransara\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "# classifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import os\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing, naive_bayes\n",
    "import string\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from utilities import read_fileNames, readFilesFromSources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create source\n",
    "sources = []\n",
    "\n",
    "datapath = './Real_Life_Trial_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file names from datapath\n",
    "read_fileNames(sources, datapath,'Deceptive')\n",
    "deceptiveCount=len(sources)\n",
    "read_fileNames(sources, datapath,'Truthful')\n",
    "truthCount=len(sources)-deceptiveCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read text files from source list\n",
    "text = []\n",
    "readFilesFromSources(text,sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create label array corresponding to text files\n",
    "labels=np.empty(len(sources))\n",
    "np.concatenate((np.ones(deceptiveCount, dtype=int),np.zeros(truthCount, dtype=int)),out=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe using texts and lables\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = text\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(trainDF['label'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25e88b1b648>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainDF['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reciprocal_rank(true_labels: list, machine_preds: list):\n",
    "    \"\"\"Compute the reciprocal rank at cutoff k\"\"\"\n",
    "    \n",
    "    # add index to list only if machine predicted label exists in true labels\n",
    "    tp_pos_list = [(idx + 1) for idx, r in enumerate(machine_preds) if r in true_labels]\n",
    "\n",
    "    rr = 0\n",
    "    if len(tp_pos_list) > 0:\n",
    "        # for RR we need position of first correct item\n",
    "        first_pos_list = tp_pos_list[0]\n",
    "        \n",
    "        # rr = 1/rank\n",
    "        rr = 1 / float(first_pos_list)\n",
    "\n",
    "    return rr\n",
    "\n",
    "def compute_mrr_at_k(items:list):\n",
    "    \"\"\"Compute the MRR (average RR) at cutoff k\"\"\"\n",
    "    rr_total = 0\n",
    "    \n",
    "    for item in items:   \n",
    "        rr_at_k = _reciprocal_rank(item[0],item[1])\n",
    "        rr_total = rr_total + rr_at_k\n",
    "        mrr = rr_total / 1/float(len(items))\n",
    "\n",
    "    return mrr\n",
    "\n",
    "def collect_preds(Y_test,Y_preds):\n",
    "    \"\"\"Collect all predictions and ground truth\"\"\"\n",
    "    \n",
    "    pred_gold_list=[[[Y_test[idx]],pred] for idx,pred in enumerate(Y_preds)]\n",
    "    return pred_gold_list\n",
    "             \n",
    "def compute_accuracy(eval_items:list):\n",
    "    correct=0\n",
    "    total=0\n",
    "    \n",
    "    for item in eval_items:\n",
    "        true_pred=item[0]\n",
    "        machine_pred=set(item[1])\n",
    "        \n",
    "        for cat in true_pred:\n",
    "            if cat in machine_pred:\n",
    "                correct+=1\n",
    "                break\n",
    "    \n",
    "    \n",
    "    accuracy=correct/float(len(eval_items))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def extract_features(df,field,training_data,testing_data,type=\"binary\"):\n",
    "    \"\"\"Extract features using different methods\"\"\"\n",
    "    \n",
    "    logging.info(\"Extracting features and creating vocabulary...\")\n",
    "    \n",
    "    if \"binary\" in type:\n",
    "        \n",
    "        # BINARY FEATURE REPRESENTATION\n",
    "        cv= CountVectorizer(binary=True, max_df=0.95)\n",
    "        cv.fit_transform(training_data[field].values)\n",
    "        \n",
    "        train_feature_set=cv.transform(training_data[field].values)\n",
    "        test_feature_set=cv.transform(testing_data[field].values)\n",
    "        \n",
    "        return train_feature_set,test_feature_set,cv\n",
    "  \n",
    "    elif \"counts\" in type:\n",
    "        \n",
    "        # COUNT BASED FEATURE REPRESENTATION\n",
    "        cv= CountVectorizer(binary=False, max_df=0.95)\n",
    "        cv.fit_transform(training_data[field].values)\n",
    "        \n",
    "        train_feature_set=cv.transform(training_data[field].values)\n",
    "        test_feature_set=cv.transform(testing_data[field].values)\n",
    "        \n",
    "        return train_feature_set,test_feature_set,cv\n",
    "    \n",
    "    else:    \n",
    "        \n",
    "        # TF-IDF BASED FEATURE REPRESENTATION\n",
    "        tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n",
    "        tfidf_vectorizer.fit_transform(training_data[field].values)\n",
    "        \n",
    "        train_feature_set=tfidf_vectorizer.transform(training_data[field].values)\n",
    "        test_feature_set=tfidf_vectorizer.transform(testing_data[field].values)\n",
    "        \n",
    "        return train_feature_set,test_feature_set,tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_predictions(model,X_test,k):\n",
    "    \n",
    "    # get probabilities instead of predicted labels, since we want to collect top 3\n",
    "    probs = model.predict_proba(X_test)\n",
    "\n",
    "    # GET TOP K PREDICTIONS BY PROB - note these are just index\n",
    "    best_n = np.argsort(probs, axis=1)[:,-k:]\n",
    "    \n",
    "    # GET CATEGORY OF PREDICTIONS\n",
    "    preds=[[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]\n",
    "    \n",
    "    preds=[ item[::-1] for item in preds]\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df,field=\"text\",feature_rep=\"binary\",top_k=3):\n",
    "    \n",
    "    logging.info(\"Starting model training...\")\n",
    "    \n",
    "    # GET A TRAIN TEST SPLIT (set seed for consistent results)\n",
    "    training_data, testing_data = train_test_split(df,test_size=0.1, random_state = 2000)\n",
    "\n",
    "    # GET LABELS\n",
    "    Y_train=training_data['label'].values\n",
    "    Y_test=testing_data['label'].values\n",
    "     \n",
    "    # GET FEATURES\n",
    "    X_train,X_test,feature_transformer=extract_features(df,field,training_data,testing_data,type=feature_rep)\n",
    "\n",
    "    # INIT LOGISTIC REGRESSION CLASSIFIER\n",
    "    logging.info(\"Training a Logistic Regression Model...\")\n",
    "    scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n",
    "    model=scikit_log_reg.fit(X_train,Y_train)\n",
    "\n",
    "    # GET TOP K PREDICTIONS\n",
    "    preds=get_top_k_predictions(model,X_test,top_k)\n",
    "    print(preds)\n",
    "    print(Y_test)\n",
    "    \n",
    "    # GET PREDICTED VALUES AND GROUND TRUTH INTO A LIST OF LISTS - for ease of evaluation\n",
    "    eval_items=collect_preds(Y_test,preds)\n",
    "    \n",
    "    # GET EVALUATION NUMBERS ON TEST SET -- HOW DID WE DO?\n",
    "    logging.info(\"Starting evaluation...\")\n",
    "    accuracy=compute_accuracy(eval_items)\n",
    "    mrr_at_k=compute_mrr_at_k(eval_items)\n",
    "    \n",
    "    logging.info(\"Done training and evaluation.\")\n",
    "    \n",
    "    return model,feature_transformer,accuracy,mrr_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-31 17:09:54,917 : INFO : Starting model training...\n",
      "2019-12-31 17:09:54,928 : INFO : Extracting features and creating vocabulary...\n",
      "2019-12-31 17:09:54,982 : INFO : Training a Logistic Regression Model...\n",
      "2019-12-31 17:09:55,001 : INFO : Starting evaluation...\n",
      "2019-12-31 17:09:55,005 : INFO : Done training and evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][[0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0]]\n",
      "[0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0.]\n",
      "\n",
      "Accuracy=0.5384615384615384; MRR=0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "field='text'\n",
    "feature_rep='tfidf'\n",
    "top_k=1\n",
    "\n",
    "model,transformer,accuracy,mrr_at_k=train_model(trainDF,field=field,feature_rep=feature_rep,top_k=top_k)\n",
    "print(\"\\nAccuracy={0}; MRR={1}\".format(accuracy,mrr_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.cnn.com/2019/07/19/politics/george-nader-child-porn-sex-charges/index.html\n",
    "test_features=transformer.transform([\"George Aref Nader, who was a key witness in special counsel Robert Mueller's Russia investigation, faces new charges of transporting a minor with intent to engage in criminal sexual activity and child pornography\"])\n",
    "get_top_k_predictions(model,test_features,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = Deceptive & 0 = Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
